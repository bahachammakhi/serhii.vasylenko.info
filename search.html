<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.7.2"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Search" /><meta name="author" content="Serhii Vasylenko" /><meta property="og:locale" content="en_US" /><meta name="description" content="Here I publish my notes about different things, mostly technical." /><meta property="og:description" content="Here I publish my notes about different things, mostly technical." /><link rel="canonical" href="https://serhii.vasylenko.info/search.html" /><meta property="og:url" content="https://serhii.vasylenko.info/search.html" /><meta property="og:site_name" content="Serhii Vasylenko" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Search" /><meta name="twitter:site" content="@vasylenko" /><meta name="twitter:creator" content="@Serhii Vasylenko" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Here I publish my notes about different things, mostly technical.","author":{"@type":"Person","name":"Serhii Vasylenko"},"@type":"WebPage","url":"https://serhii.vasylenko.info/search.html","headline":"Search","@context":"https://schema.org"}</script><title>Search | Serhii Vasylenko</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/page.css" as="style"><link rel="stylesheet" href="/assets/css/page.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/page.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Serhii Vasylenko</a></div><div class="site-subtitle font-italic">Personal blog</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/tabs/categories.html" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tabs/tags.html" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/tabs/archives.html" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/tabs/about.html" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/vasylenko" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/vasylenko" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['mail-from-blog','vasylenko.info'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span>Search</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Page</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="page" class="post pb-5 pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4 mb-md-4"><div class="search"><div id="search-results"></div><p id="not-found" style="display: none"></p></div><script> window.store = { "2021-05-27-run-ansible-playbook-mac1-metal-aws-systems-manager-html": { "title": "Run Ansible playbook on mac1.metal instances fleet with AWS Systems Manager", "tags": "aws, ansible", "date": "May 27, 2021", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "In days of containers and serverless applications, Ansible looks not such a trendy thing.But still, there are cases when it helps, and there are cases when it combines very well with brand new product offerings, such as EC2 Mac instances.And the more I use mac1.metal in AWS, the more I see that Ansible becomes a bedrock of software customization in my case.(Not) Available out of the boxAWS SSM has a pre-defined, shared Document that allows running Ansible playbooks. It’s called “AWS-RunAnsiblePlaybook,” and you can find it in AWS Systems Manager -&gt; Documents -&gt; Owned by Amazon.It is not quite “friendly” to macOS: when the SSM agent tries to use Ansible, it does not recognize the Ansible installed with Homebrew (de-facto most used macOS package manager).So if you try to run a command on the mac1.metal instance using this Document, you will get the following error:1Ansible is not installed. Please install Ansible and rerun the command.The root cause is simple: the Ansible binary path is not present on the list of paths available to the SSM agent by default.There are several ways to solve that, but I believe that the most convenient one would be to create your custom Document — a slightly adjusted version of the default provided by AWS.Creating own SSM Document for Ansible installed with HomebrewThe solution is simple: all you need to do is clone the Document provided by AWS and change its code a little — replace the callouts of ansible with the full path to the binary.Navigate to AWS Systems Manager -&gt; Documents -&gt; Owned by Amazon and type AWS-RunAnsiblePlaybook in the search field.Select the Document by pressing the circle on its top-right corner and then click Actions -&gt; Clone document.Give the new SSM Document a name, i.e. macos-arbitrary-ansible-playbook, and change the ansible callouts (at the end of the code) with the full path to the ansible symlink made by Homebrew which is /usr/local/bin/ansibleHere is the complete source code of the Document with adjusted Ansible path:Click to see the Document source code{ \"schemaVersion\": \"2.0\", \"description\": \"Use this document to run arbitrary Ansible playbooks on macOS EC2 instances. Specify either YAML text or URL. If you specify both, the URL parameter will be used. Use the extravar parameter to send runtime variables to the Ansible execution. Use the check parameter to perform a dry run of the Ansible execution. The output of the dry run shows the changes that will be made when the playbook is executed.\", \"parameters\": { \"playbook\": { \"type\": \"String\", \"description\": \"(Optional) If you don't specify a URL, then you must specify playbook YAML in this field.\", \"default\": \"\", \"displayType\": \"textarea\" }, \"playbookurl\": { \"type\": \"String\", \"description\": \"(Optional) If you don't specify playbook YAML, then you must specify a URL where the playbook is stored. You can specify the URL in the following formats: http://example.com/playbook.yml or s3://examplebucket/plabook.url. For security reasons, you can't specify a URL with quotes.\", \"default\": \"\", \"allowedPattern\": \"^\\\\s*$|^(http|https|s3)://[^']*$\" }, \"extravars\": { \"type\": \"String\", \"description\": \"(Optional) Additional variables to pass to Ansible at runtime. Enter a space separated list of key/value pairs. For example: color=red or fruits=[apples,pears]\", \"default\": \"foo=bar\", \"displayType\": \"textarea\", \"allowedPattern\": \"^((^|\\\\s)\\\\w+=(\\\\S+|'.*'))*$\" }, \"check\": { \"type\": \"String\", \"description\": \" (Optional) Use the check parameter to perform a dry run of the Ansible execution.\", \"allowedValues\": [ \"True\", \"False\" ], \"default\": \"False\" }, \"timeoutSeconds\": { \"type\": \"String\", \"description\": \"(Optional) The time in seconds for a command to be completed before it is considered to have failed.\", \"default\": \"3600\" } }, \"mainSteps\": [ { \"action\": \"aws:runShellScript\", \"name\": \"runShellScript\", \"inputs\": { \"timeoutSeconds\": \"\", \"runCommand\": [ \"#!/bin/bash\", \"/usr/local/bin/ansible --version\", \"if [ $? -ne 0 ]; then\", \" echo \\\"Ansible is not installed. Please install Ansible and rerun the command\\\" &gt;&amp;2\", \" exit 1\", \"fi\", \"execdir=$(dirname $0)\", \"cd $execdir\", \"if [ -z '' ] ; then\", \" if [[ \\\"\\\" == http* ]]; then\", \" wget '' -O playbook.yml\", \" if [ $? -ne 0 ]; then\", \" echo \\\"There was a problem downloading the playbook. Make sure the URL is correct and that the playbook exists.\\\" &gt;&amp;2\", \" exit 1\", \" fi\", \" elif [[ \\\"\\\" == s3* ]] ; then\", \" aws --version\", \" if [ $? -ne 0 ]; then\", \" echo \\\"The AWS CLI is not installed. The CLI is required to process Amazon S3 URLs. Install the AWS CLI and run the command again.\\\" &gt;&amp;2\", \" exit 1\", \" fi\", \" aws s3 cp '' playbook.yml\", \" if [ $? -ne 0 ]; then\", \" echo \\\"Error while downloading the document from S3\\\" &gt;&amp;2\", \" exit 1\", \" fi\", \" else\", \" echo \\\"The playbook URL is not valid. Verify the URL and try again.\\\"\", \" fi\", \"else\", \" echo '' &gt; playbook.yml\", \"fi\", \"if [[ \\\"\\\" == True ]] ; then\", \" /usr/local/bin/ansible-playbook -i \\\"localhost,\\\" --check -c local -e \\\"\\\" playbook.yml\", \"else\", \" /usr/local/bin/ansible-playbook -i \\\"localhost,\\\" -c local -e \\\"\\\" playbook.yml\", \"fi\" ] } } ]}Applying Ansible playbook to the fleet of mac1.metalLet’s give our new SSM Document a try! (I suppose you have at least one mac1 instance running, right?)In AWS Systems Manager, go to the Run Command feature, then click on the Run Command button.On the new panel, type the name of your Document (macos-arbitrary-ansible-playbook in this example) in the search field and press enter.Select the Document, and you’ll see its parameters and settings.The rest is self-explanatory. Enter either a playbook code or a link to the source file, add extra variables if needed, and select the target host or a filtered bunch (I like that feature with tags filtering!). Finally, click on the “Run” orange button to apply your playbook.That’s it! Now you can make all your ansible-playbook dreams come true! 😁", "url": "//2021/05/27/run-ansible-playbook-mac1-metal-aws-systems-manager.html" } , "2021-05-21-configure-http-security-headers-with-cloudfront-functions-html": { "title": "Configure HTTP Security headers with CloudFront Functions", "tags": "aws, cloudfront", "date": "May 21, 2021", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "A couple of weeks ago, AWS released CloudFront Functions — a “true edge” compute capability for the CloudFront.It is “true edge” because Functions work on 200+ edge locations (link to doc) while its predecessor, the Lambda@Edge, runs on a small number of regional edge caches.One of the use cases for Lambda@Edge was adding security HTTP headers (it’s even listed on the product page), and now there is one more way to make it using CloudFront Functions.What are security headers, and why it mattersSecurity Headers are one of the web security pillars.They specify security-related information of communication between a web application (i.e., website) and a client (i.e., browser) and protect the web app from different types of attacks. Also, HIPAA and PCI, and other security standard certifications generally include these headers in their rankings.We will use CloudFront Functions to set the following headers: Content Security Policy Strict Transport Security X-Content-Type-Options X-XSS-Protection X-Frame-Options Referrer PolicyYou can find a short and detailed explanation for each security header on Web Security cheatsheet made by MozillaCloudFront Functions overviewIn a nutshell, CloudFront Functions allow performing simple actions against HTTP(s) request (from the client) and response (from the CloudFront cache at the edge). Functions take less than one millisecond to execute, support JavaScript (ECMAScript 5.1 compliant), and cost $0.10 per 1 million invocations.Every CloudFront distribution has one (default) or more Cache behaviors, and Functions can be associated with these behaviors to execute upon a specific event.That is how the request flow looks like in general, and here is where CloudFront Functions execution happens:CloudFront Functions support Viewer Request (after CloudFront receives a request from a client) and Viewer Response (before CloudFront forwards the response to the client) events.You can read more about the events types and their properties here — CloudFront Events That Can Trigger a Lambda Function - Amazon CloudFront.Also, the CloudFront Functions allow you to manage and operate the code and lifecycle of the functions directly from the CloudFront web interface.Solution overviewCloudFront distribution should exist before Function creation so you could associate the Function with the distribution.Creation and configuration of the CloudFront Function consist of the following steps:Create FunctionIn the AWS Console, open CloudFront service and lick on the Functions on the left navigation bar, then click Create function button. Enter the name of your Function (e.g., “security-headers”) and click Continue.Build FunctionOn the function settings page, you will see four tabs with the four lifecycle steps: Build, Test, Publish, Associate.Paste the function code into the editor and click “Save.”Here is the source code of the function:12345678910111213function handler(event) {var response = event.response;var headers = response.headers;headers['strict-transport-security'] = { value: 'max-age=63072000; includeSubdomains; preload'}; headers['content-security-policy'] = { value: \"default-src 'none'; img-src 'self'; script-src 'self'; style-src 'self'; object-src 'none'; frame-ancestors 'none'\"}; headers['x-content-type-options'] = { value: 'nosniff'}; headers['x-xss-protection'] = {value: '1; mode=block'};headers['referrer-policy'] = {value: 'same-origin'};headers['x-frame-options'] = {value: 'DENY'};return response;}Test FunctionOpen the “Test” tab — let’s try our function first before it becomes live!Select Viewer Response event type and Development Stage, then select “Viewer response with headers” as a Sample test event (you will get a simple set of headers automatically).Now click the blue “Test” button and observe the output results: Compute utilization represents the relative amount of time (on a scale between 0 and 100) your function took to run Check the Response headers tab and take a look at how the function added custom headers.Publish FunctionLet’s publish our function. To do that, open the Publish tab and click on the blue button “Publish and update.”Associate your Function with CloudFront distributionNow, you can associate the function with the CloudFront distribution.To do so, open the Associate tab, select the distribution and event type (Viewer Response), and select the Cache behavior of your distribution which you want to use for the association.Once you associate the function with the CloudFront distribution, you can test it in live mode.I will use curl here to demonstrate it:12345678910111213141516171819&gt; curl -i https://d30i87a4ss9ifz.cloudfront.netHTTP/2 200content-type: text/htmlcontent-length: 140date: Sat, 22 May 2021 00:22:18 GMTlast-modified: Tue, 27 Apr 2021 23:07:14 GMTetag: \"a855a3189f8223db53df8a0ca362dd62\"accept-ranges: bytesserver: AmazonS3via: 1.1 50f21cb925e6471490e080147e252d7d.cloudfront.net (CloudFront)content-security-policy: default-src 'none'; img-src 'self'; script-src 'self'; style-src 'self'; object-src 'none'; frame-ancestors 'none'strict-transport-security: max-age=63072000; includeSubdomains; preloadx-xss-protection: 1; mode=blockx-frame-options: DENYreferrer-policy: same-originx-content-type-options: nosniffx-cache: Miss from cloudfrontx-amz-cf-pop: WAW50-C1x-amz-cf-id: ud3qH8rLs7QmbhUZ-DeupGwFhWLpKDSD59vr7uWC65Hui5m2U8o2mw==You can also test your results here — Mozilla ObservatoryRead moreThat was a simplified overview of the CloudFront Functions capabilities.But if you want to get deeper, here is a couple of useful links to start: Another overview from AWS — CloudFront Functions Launch Blog More about creating, testing, updating and publishing of CloudFront Functions — Managing functions in CloudFront Functions - Amazon CloudFrontSo what to choose?CloudFront Functions are simpler than Lambda@Edge and run faster with minimal latency and minimal time penalty for your web clients.Lambda@Edge takes more time to invoke, but it can run upon Origin Response event so that CloudFront can cache the processed response (including headers) and return it faster afterward.But again, the CloudFront Functions invocations are much cheaper (6x times) than Lambda@Edge, and you do not pay for the function execution duration.The final decision would also depend on the dynamic/static nature of the content you have at your origin.To make a wise and deliberate decision, try to analyze your use case using these two documentation articles: Choosing between CloudFront Functions and Lambda@Edge How to Decide Which CloudFront Event to Use to Trigger a Lambda Function", "url": "//2021/05/21/configure-http-security-headers-with-cloudfront-functions.html" } , "2021-02-14-image-compression-with-tinypng-from-macos-contextual-menu-html": { "title": "Using TinyPNG Image Compression From MacOS Finder Contextual Menu", "tags": "macos, automation, fun", "date": "February 14, 2021", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "I just wanted to compress one image, but went to far…or “How to add TinyPNG image compression to your macOS Finder contextual menu.”What is it and how it worksYou select needed files or folders, then right-click on them, click on the Services menu item and choose TinyPNG.After a moment, the new optimized versions of images will appear near to original files.If you selected a folder along with the files, the script would process all png and jpeg files in it.PrerequisitesYou need to register at TinyPNG and get your API key here — Developer API.They sometimes block some countries (for example, Ukraine) from registration; in that case, try to use a web-proxy or VPN.How to create Quick Action WorkflowOpen Automator application. If you never used this app before, please read about it on the official user guide website.On the New Action screen, chose Quick ActionAfter you click the “Choose” button, you’ll see the workflow configuration window.Workflow configurationFind the Run Shell Script action on the Utilities list in Library on the left, and drag it onto the right side of the panel.Set the following workflow configuration options as described below:Workflow receives current files and folders in FinderShell /bin/zshPass input as argumentsClick the Option button at the bottom of the Action window and Uncheck Show this action when the workflow runs.Put the following script into the Run Shell Script window, replacing the YOUR_API_KEY_HERE string with your API key obtained from TinyPNG.400: Invalid requestUtilities used in the script — explainedcurl — used to make web requests (like your browser does)grep — used to parse the response for the needed header (i.e., field) with the file download linkcut — used to extract the URL from the parsed resultsed — used to remove the trailing “carriage return” symbol at the end of extracted stringThe response body also contains a JSON object that includes the download URL; you can parse it with jq, for example. But I intentionally refused to use the jq tool because it is not pre-installed in MacOS.ConclusionIt is simple, and it does its job fine. And you don’t need to install anything to make it work.To make this a bit fancier, you might also like to add a “Display Notification” (from the Utilities library on the left) after the “Run Shell Script”. The action will display a notification once image processing is completed.Thank you for reading!", "url": "//2021/02/14/image-compression-with-tinypng-from-macos-contextual-menu.html" } , "2021-02-01-customizing-mac1-metal-ec2-ami-html": { "title": "Customizing mac1.metal EC2 AMI — new guts, more glory", "tags": "aws, ansible, packer, automation, devops, macos", "date": "February 1, 2021", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "I guess macOS was designed for a user, not for the ops or engineers, so this is why its customization and usage for CI/CD are not trivial (compared to something Linux-based). A smart guess, huh?Configuration ManagementNative Apple’s Mobile device management (a.k.a MDM) and Jamf is probably the most potent combination for macOS configuration. But as much as it’s mighty, it is a cumbersome combination, and Jamf is not free.Then we have Ansible, Chef, Puppet, SaltStack — they all are good with Linux, but what about macOS?I tried to search for use cases of mentioned CM tools for macOS. However, I concluded that they wrap the execution of native macOS command-line utilities most of the time.And if you search for the ‘macos’ word in Chef Supermarket or Puppet Forge, you won’t be impressed by the number of actively maintained packages. Although, here is a motivating article about using Chef automating-macos-provisioning-with-chef if you prefer it. I could not find something similar and fresh for Puppet, so I am sorry, Puppet fans.That is why I decided to follow the KISS principle and chose Ansible.It’s easy to write and read the configuration, it allows to group tasks and to add execution logic , and it feels more DevOps executing shell commands inside Ansible tasks instead of shell scripts; I know you know that 😂By the way, Ansible Galaxy does not have many management packages for macOS, either. But thankfully, it has the basics: homebrew with homebrew_cask and homebrew_tap — to install software launchd — to manage services osx_defaults — to manage some user settings (not all!)I used Ansible to build the macOS AMI for CI/CD, so here are some tips for such a case.Some values are hardcoded intentionally in the code examples for the sake of simplicity and easy reading. You would probably want to parametrize them.Xcode installation exampleThe following tasks will help you to automate the basics.12345678910111213- name: Install Xcode shell: \"xip --expand Xcode.xip\" args: chdir: /Applications- name: Accept License Agreement shell: \"/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -license accept\"- name: Accept License Agreement shell: \"/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -runFirstLaunch\"- name: Switch into newly installed Xcode context shell: \"xcode-select --switch /Applications/Xcode.app/Contents/Developer\"Example of software installation with Brew12345678- name: Install common build software community.general.homebrew: name: \"{{ item }}\" state: latest loop: - swiftlint - swiftformat - wgetScreenSharing (remote desktop) configuration example123456789- name: Turn On Remote Management shell: \"./kickstart -activate -configure -allowAccessFor -specifiedUsers\" args: chdir: /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/- name: Enable Remote Management for CI user shell: \"./kickstart -configure -users ec2-user -access -on -privs -all\" args: chdir: /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/Shell rulez, yes.Building the AMIPacker by HashiCorp, of course.I would love to compare Packer with EC2 Image Builder, but it does not support macOS yet (as of Feb’21).Packer configuration is straightforward, so I want to highlight only the things specific to the “mac1.metal” use case.TimeoutsAs I mentioned in the previous article, the creation and deletion time of the “mac1.metal” Instance is significantly bigger than Linux. That is why you should raise the polling parameters for the builder.Example:1234\"aws_polling\": { \"delay_seconds\": 30, \"max_attempts\": 60}And it would be best if you also increased the SSH timeout:1 \"ssh_timeout\": \"1h\"Fortunately, Packer’s AMI builder does not require an explicit declaration of the Dedicated Host ID. So you can just reference the same subnet where you allocated the Host, assuming you did it with the enabled “Auto placement” parameter during the host creation.Example:12 \"tenancy\": \"host\", \"subnet_id\": \"your-subnet-id\"ProvisioningPacker has Ansible Provisioner that I used for the AMI. Its documentation is also very clean and straightforward.But it is still worth mentioning that if you want to parametrize the Ansible playbook, then the following configuration example will be handy:12345678 \"extra_arguments\": [ \"--extra-vars\", \"your-variable-foo=your-value-bar]\" ], \"ansible_env_vars\": [ \"ANSIBLE_PYTHON_INTERPRETER=auto_legacy_silent\", \"ANSIBLE_OTHER_ENV_VARIABLE=other_value\" ]Configuration at launchIf you’re familiar with AWS EC2, you probably know what the Instance user data is.A group of AWS developers made something similar for the macOS: EC2 macOS Init.It does not support cloud-init as on Linux-based Instances, but it can run shell scripts, which is quite enough.EC2 macOS Init utility is a Launch Daemon (macOS terminology) that runs on behalf of the root user at system boot. It executes the commands according to the so-called Priority Groups, or the sequence in other words.The number of the group corresponds to the execution order. You can put several tasks into a single Priority Group, and the tool will execute them simultaneously.EC2 macOS Init uses a human-readable configuration file in toml format.Example:123456789[[Module]] Name = \"Create-some-folder\" PriorityGroup = 3 FatalOnError = false RunPerInstance = true [Module.Command] Cmd = [\"mkdir\", \"/Users/ec2-user/my-directory\"] RunAsUser = \"ec2-user\" EnvironmentVars = [\"MY_VAR_FOO=myValueBar\"]I should clarify some things here.Modules — a set of pre-defined modules for different purposes. It is something similar to the Ansible modules.You can find the list of available modules here ec2-macos-init/lib/ec2macosinitThe RunPerInstance directive controls whether a module should run. There are three of such directives, and here is what they mean: RunPerBoot — module will run at every system boot RunPerInstance — module will run once for the Instance. Each Instance has a unique ID; the init tool fetches it from the AWS API before the execution and keeps its execution history per Instance ID. When you create a new Instance from the AMI, it will have a unique ID, and the module will run again. RunOnce — module will run only once, despite the instance ID changeI mentioned the execution history above. When EC2 macOS Init runs on the Instance first time, it creates a unique directory with the name per Instance ID to store the execution history and user data copy.RunPerInstance and RunOnce directives depend on the execution history, and modules with those directives will run again on the next boot if the previous execution failed. It was not obvious to me why RunOnce keeps repeating itself every boot until I dug into the source code.Finally, there is a module for user data. It runs at the end by default (priority group #4) and pulls the user data script from AWS API before script execution.I suggest looking into the default init.toml configuration file to get yourself more familiar with the capabilities of the tool.The init tool can also clear its history, which is useful for the new AMI creation.Example:1ec2-macos-init clean -allAnd you can run the init manually for debugging purposes.Example:1ec2-macos-init runYou can also combine the EC2 macOS Init actions (made by modules) with your script in user data for more accurate nontrivial configurations.Wrapping upAs a whole, building and operating macOS-based AMI does not differ from AMI management for other platforms.There are the same principle stages: prepare, clear, build, execute deployment script (if necessary). Though, the particular implementation of each step has its nuances and constraints.So the whole process may look as follows: Provision and configure needed software with Ansible playbook Clean-up system logs and EC2 macOS Init history (again, with Ansible task) Create the AMI Add more customizations at launch with EC2 macOS Init modules and user data (that also executes your Ansible playbook or shell commands)Getting into all this was both fun and interesting. Sometimes painful, though. 😆I sincerely hope this article was helpful to you. Thank you for reading!", "url": "//2021/02/01/customizing-mac1-metal-ec2-ami.html" } , "2021-01-20-terraforming-mac1-metal-at-aws-html": { "title": "Terraforming mac1.metal at AWS", "tags": "aws, terraform, automation, devops", "date": "January 20, 2021", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "Recently AWS announced the support for Mac mini instances.I believe this is huge, even despite the many constraints this solution has. Oh, and the price is as huge as the announcement as well.But still, this offering opens the door to seamless macOS CI/CD integration into existing AWS infrastructure.Here is a tip for engineers like me who decided to give this new instance type a try: managing a dedicated host for the “mac1.metal” instance using Terraform.“mac1.metal” instance requires a dedicated host to be placed onto. This is a real Mac mini with a bit of magic from AWS.As of 10 Jan 2021, the AWS Terraform provider does not have a dedicated host resource.So we must solve this using CloudFormation… in Terraform!1234567891011121314151617181920212223242526272829303132333435363738394041variable \"availability_zone\" { default = \"us-east-1a\"}resource \"random_pet\" \"runner_name\" { length = 2 prefix = \"mac-metal-\" separator = \"-\"}resource \"aws_cloudformation_stack\" \"dedicated_host\" { name = random_pet.runner_name.id timeout_in_minutes = 20 template_body = &lt;&lt;STACK{ \"Resources\" : { \"MyDedicatedHost\": { \"Type\" : \"AWS::EC2::Host\", \"Properties\" : { \"AutoPlacement\" : \"on\", \"AvailabilityZone\" : \"${var.availability_zone}\", \"HostRecovery\" : \"off\", \"InstanceType\" : \"mac1.metal\" } } }, \"Outputs\" : { \"HostID\" : { \"Description\": \"Host ID\", \"Value\" : { \"Ref\" : \"MyDedicatedHost\" } } }}STACK}output \"dedicated_host_id\" { value = aws_cloudformation_stack.dedicated_host.outputs[\"HostID\"]}Then you pass HostID to aws_instance resource, and you have mac1.metal up and running!I like to make boring technical things a bit less boring, so I decided to add a random pet name. Just for fun, why not.You can wrap this into a module or add a count meta-argument to make it more versatile.Simple as that, yes. But now, you can integrate it into your CI system (if you have enough courage and money 😄) and have the Mac instance with the underlying host in a bundle.Thanks for reading me!credits for cover image: AWS EC2 Mac Instances Launch - macOS in the cloud for the first time, with the benefits of EC2", "url": "//2021/01/20/terraforming-mac1-metal-at-AWS.html" } , "2021-01-19-mac1-metal-ec2-instance-user-experience-html": { "title": "mac1.metal EC2 Instance — user experience", "tags": "aws, macos", "date": "January 19, 2021", "author": "", "category": "[&quot;Review&quot;]", "content": "Amazon EC2 Mac InstancesSomething cool and powerful with inevitable trade-offs. As everything in this world.AWS announced EC2 macOS-based instances on the 30th of November 2020, and after more than a month of tests, I would like to share some findings and impressions about it.First of all, the things you can easily find, but it’s still worth to say: The new instance family is called mac1.metal. Guess we should expect mac2 or mac3; otherwise, why did they put a number in the name? They added AWS Nitro System to integrate them with many AWS services. The Instance must be placed onto a Dedicated Host. Only one Instance per Host is allowed because the Host is an actual Mac Mini in that case. You don’t pay anything for the Instance itself, but you pay for the Dedicated Host leasing — $1.083, and the minimum lease time is 24 hours. So the launch of the “mac1.metal” Instance costs $26 at minimum. Prices provided for the cheapest region — North Virginia. You can apply Saving Plans to save some money. Mojave (10.14) and Catalina (10.15) are supported at the moment, with “support for macOS Big Sur (11.0) coming soon”. I expect it to be in 2021, though.What can it doHere is a list of some features that the “mac1.metal” instance has: It lives in your VPC because it is an EC2 Instance so that you can access many other services. It supports the new gp3 EBS type (and other types as well). It supports SSM Agent and Session Manager. It has several AWS tools pre-installed. It has pre-installed Enhanced Network Interface drivers. My test upload/download to S3 was about 300GB/s. It can report CPU metrics to CloudWatch (if you ever need it, though).What can’t it do It can’t be used in Auto Scaling because of a Dedicated Host. It can’t recognize the attached EBS if you connected it while the Instance was running — you must reboot the Instance to make it visible. It does not support several services that rely on additional custom software, such as “EC2 Instance Connect” and “AWS Inspect.” But I think that AWS will add macOS distros for those soon.Launching the InstanceJeff Bar published an excellent how-to about kickstart of the “mac1.metal”, so I will focus on things he did not mention.Once you allocated the Dedicated Host and launched an Instance on it, the underlying system connects the EBS with a root file system to the Mac Mini.It is an AMI with 32G EBS (as per Jan’21) with macOS pre-installed.That means two things: The built-it physical SSD is still there and still yours to use; however, AWS does not manage or support the Apple hardware’s internal SSD. You must resize the disk manually (if you specified the EBS size to be more than 32G)[1].The time from the Instance launch till you’re able to SSH into it varies between 15 and 20 minutes.You have the option to access it over SSH with your private key. If you need to set up Screen Sharing, you have to allow it through the “kickstart” command-line utility and setting the user password [2].Destroying the InstanceWhat an easy thing to do, right? Well, it depends.When you click on the “Terminate” item in the Instance actions menu, the complex Instance scrubbing process begins.AWS wants to make sure that anyone who uses the Host (Mac mini) after you will get your data stored neither on disks (including physical SSD mentioned earlier), nor inside memory or NVRAM, nor anywhere else. They do not share the info about this scrubbing process’s details, but it takes more than an hour to complete.When scrubbing is started, the Dedicated Host transitions to the Pending state. Dedicated Host transitions to Available state once scrubbing is finished. But you must wait for another 10-15 minutes to be able to release it finally.I don’t know why they set the Available state value earlier than the Host is available for operations, but this is how it works now (Jan’21).Therefore, you can launch the next Instance on the same Host not earlier than ~1,5 hours after you terminated the previous. That doesn’t seem very pleasant in the first couple of weeks, but you will get used to it. 😄And again: you can release the “mac1.metal” Dedicated Host not earlier than 24 hours after it was allocated. So plan your tests wisely.Legal thingsI could not find it on a documentation page, but A Cloud Guru folks say that you must use new Instances solely for developer services, and you must agree to all of the EULAs.Sounds reasonable to me, but that could be written somewhere in the docs still, at least. Please let me know if you found it there.Some more cool stuff to check:EC2 macOS Init launch daemon, which is used to initialize Mac instances.EC2 macOS Homebrew Tap (Third-Party Repository) with several management tools which come pre-installed into macOS AMI from AWS.Indeed it is powerful, and it has its trade-offs, such as price and some technical constraints. But it is a real MacOS device natively integrated into the AWS environment. So I guess it worth to be tried!Thanks for reading this! Stay tuned for more user experience feedback about baking custom AMI’s, automated software provisioning with Ansible, and other adventures with mac1.metal![1] How to resize the EBS at mac1.metal in TerminalGet the identifier of EBS (look for the first one with GUID_partition_scheme):diskutil list physical externalOr here is a more advanced version to be used in a script:1DISK_ID=$(diskutil list physical external | grep 'GUID_partition_scheme'| tr -s ' ' | cut -d' ' -f6)It would probably be disk0 if you did not attach additional EBS.Then run the repair job for the disk, using its identifier:diskutil repairDisk disk0Advanced version:1yes | diskutil repairDisk $DISK_IDNow get the APFS container identifier (look for Apple_APFS):diskutil list physical externalAdvanced version:1APFS_ID=$(diskutil list physical external | grep 'Apple_APFS' | tr -s ' ' | cut -d' ' -f8)It would probably be disk0s2 if you did not attach additional EBS.Finally, resize the APFS container:diskutil apfs resizeContainer disk0s2Advanced version1diskutil apfs resizeContainer $APFS_ID[2]How to setup Screen Sharing at mac1.metal in TerminalThe kickstart command-line tool resides in /System/Library/CoreServices/RemoteManagement/ARDAgent.app/Contents/Resources/ so you’ll better to cd into that directory for convenience:12345678# Turn On Remote Management for a user to be specified latersudo ./kickstart -activate -configure -allowAccessFor -specifiedUsers# Enable Remote Management for ec2-user usersudo ./kickstart -configure -users ec2-user -access -on -privs -all# Set the user password sudo passwd ec2-user", "url": "//2021/01/19/mac1-metal-EC2-Instance-user-experience.html" } , "2020-12-16-aws-cloudshell-html": { "title": "AWS CloudShell", "tags": "aws", "date": "December 16, 2020", "author": "", "category": "[&quot;Review&quot;]", "content": "A simple but cool announcement from AWS — AWS CloudShell.A tool for ad-hoc AWS management via CLI directly in your browser.I like when AWS releases something simple to understand and yet powerful.So it is not another DevOps Guru, believe me :) Yes, this is similar to the shells that GCE and Azure have. No, you can’t access your instances from it, so it’s not a jump server (bastion host). Yes, it has AWS CLI and other tools pre-installed. Even Python and Node.js. No, you can’t (well, you can, but should not) use it as an alternative to the day-to-day console on your laptop. Yes, you can manage all resources from that shell as much as your IAM permissions allow you (even with SSO, which is pretty cool). No, it does not support Docker. Yes, you have 1 GB of permanent storage and the ability to transfer files in and out.More Yes and No’s here:https://docs.aws.amazon.com/cloudshell/latest/userguide/faq-list.htmlhttps://aws.amazon.com/cloudshell/faqs/", "url": "//2020/12/16/aws-cloudshell.html" } , "2020-09-16-terraform-workflow-working-individually-and-in-a-team-html": { "title": "Terraform Workflow — Working Individually and in a Team", "tags": "terraform", "date": "September 16, 2020", "author": "", "category": "[&quot;Learning&quot;]", "content": "The work with Terraform code may become tangled sometimes. Here are some guides on how to streamline it and make it transparent for you and your team.It is extremely helpful in a team, and can benefit you even if you work individually. A good workflow enables you to streamline a process, organize it, and make it less error-prone.This article summaries several approaches when working with Terraform, both individually and in a team. I tried to gather the most common ones, but you might also want to develop your own.The common requirement for all of them is a version control system (such as Git). This is how you ensure nothing is lost and all your code changes are properly versioned tracked.Table of contents: Basic Concepts Core individual workflow Core team workflow Team workflow with automation Import workflowBasic ConceptsLet’s define the basic actions first.All described workflows are built on top of three key steps: Write, Plan, and Apply. Nevertheless, their details and actions vary between workflows.It’s a piece of cake, isn’t it? 😆Write – this is where you make changes to the code.Plan – this is where you review changes and decide whether to accept them.Apply – this is where you accept changes and apply them against real infrastructure.It’s a simple idea with a variety of possible implementations.Core individual workflowThis is the most simple workflow if you work alone on a relatively small TF project. This workflow suits both local and remote backends well.Let’s add a bit of GitWriteYou clone the remote code repo or pull the latest changes, edit the configuration code, then run the terraform validate and terraform fmt commands to make sure your code works well.PlanThis is where you run the terraform plan command to make sure that your changes do what you need. This is a good time to commit your code changes changes (or you can do it in the next step).ApplyThis is when you run terraform apply and introduce the changes to real infrastructure objects. Also, this is when you push committed changes to the remote repository.Core team workflowThis workflow is good for when you work with configuration code in a team and want to use feature branches to manage the changes accurately.Don’t get scared, it is still simple, just follow the linesWriteStart by checking out a new branch, make your changes, and run the terraform validate and terraform fmt commands to make sure your code works well.Running terraform plan at this step will help ensure that you’ll get what you expect.PlanThis is where code and plan reviews happen.Add the output of the terraform plan command to the Pull Request with your changes. It would be a good idea to add only the changed parts of the common output, which is the part that starts with “Terraform will perform the following actions” string.ApplyOnce the PR is reviewed and merged to the upstream branch, it is safe to finally pull the upstream branch locally and apply the configuration with terraform apply.Team workflow with automationIn a nutshell, this workflow allows you to introduce a kind of smoke test for your infrastructure code (using plan) and also to automate the feedback in the CI process.The automated part of this workflow consists of a speculative plan on commit and/or Pull Request (PR ), along with adding the output of plan to the comment of the PR. A speculative plan mean just to show the changes, and not apply them afterward.I like when TF plan output is included to PR, but nobody likes to read others TF plans for some reason…WriteThis step is the same as in the previous workflow.PlanThis is where your CI tool does its job.Let’s review this step by step: You create a PR with the code changes you wish to implement. The CI pipeline is triggered by an event from your code repository (such as webhook push) and it runs a speculative plan against your code. The list of changes (a so-called “plan diff”) is added to PR for review by the CI. Once merged, the CI pipeline runs again and you get the final plan that’s ready to be applied to the infrastructure.ApplyNow that you have a branch (i.e. main) with the fresh code to apply, you need to pull it locally and run terraform apply.You can also add the automated apply here – step 5 in the picture below. This may be very useful for disposable environments such as testing, staging, development, and so on.The exact CI tool to be used here is up to you: Jenkins, GitHub Actions, and Travis CI all work well.An important thing to note is that the CI pipeline must be configured in a bi-directional way with your repository to get the code from it and report back with comments to PR.As an option, you may consider using Terraform Cloud which has a lot of functionality, including the above mentioned repo integration, even with the free subscription.If you have never worked with Terraform Cloud before and want to advice to get started, I’ll provide the links at the end of this article.Import workflowThis workflow refers to a situation when you have some objects already created (i.e., up and running), and you need to manage them with Terraform.Suppose we already have an S3 bucket in AWS called “someassetsbucket” and we want to include it into our configuration code.‌‌PrepareYou should create a resource block to be used later for the real object you’re going to import.You don’t need to fill the arguments in it at the start, so it may be just a blank resource block, for example:123resource \"aws_s3_bucket\" \"someassetsbucket\" {‌‌}ImportNow you need to import the information about the real object into your existing Terraform state file.This can be done with the terraform import command, for example:1terraform import aws_s3_bucket.assets \"someassetsbucket\"‌Be sure to also check the list of possible options import accepts with terraform import -hWriteNow you need to write the corresponding Terraform code for this bucket.To avoid modifying your real object on the terraform apply action, you should specify all needed arguments with the exact values from the import phase.You can see the details by running the terraform state show command, for example:1terraform state show aws_s3_bucket.assetsThe output of this command will be very similar to the configuration code. But it contains both arguments and attributes of the resource, so you need to clean it up before applying it.You can use one of the following tactics: either copy/paste it, and then run terraform validate and terraform plan several times to make sure there are no errors like “argument is not expected here” or “this field cannot be set” or you can pick and write only the necessary argumentsIn any case, be sure to refer to the documentation of the resource during this process.PlanThe goal is to have a terraform plan output showing “~ update in-place” changes only.However, it is not always clear whether the real object will be modified or only the state file will be updated. This is why you should understand how a real object works and know its life cycle to make sure it is safe to apply the plan.ApplyThis is usual the terraform apply action.Once applied, your configuration and state file will correspond to the real object configuration.Wrapping upHere is an overview of Terraform Cloud for those who never worked with it before: ‌‌Overview of Terraform Cloud FeaturesAnd here is a nice tutorial to start with: Get Started - Terraform CloudAlso, here is an overview of workflows at scale from the HashiCorp CTO which might be useful for more experienced Terraform users: Terraform Workflow Best Practices at ScaleThank you for reading. I hope you will try one of these workflows, or develop your own!‌‌This article was originaly published on FreeCodeCamp paltform by me, but I still want to keep it here for the record. Canonical link to original publication was properly set in the page headers.", "url": "//2020/09/16/terraform-workflow-working-individually-and-in-a-team.html" } , "2020-09-15-terraform-certification-tips-html": { "title": "Terraform Certification Tips", "tags": "terraform", "date": "September 15, 2020", "author": "", "category": "[&quot;Learning&quot;]", "content": "I successfully passed the “HashiCorp Certified — Terraform Associate” exam last Friday and decided to share some advice for exam preparation.Make yourself a planMake a list of things you are going to go through: links to the study materials, practice tasks, some labs, some articles on relative blogs (Medium, Dev.to, etc.).It should look at a “todo” or “check”-list. It may seem silly at first glance, but the list with checkboxes does its “cognitive magic”. When you go point by point, marking items as “done”, you feel the progress and this motivates you to keep going further.For example, you can make a plan from the resources I outlined below in this article.I encourage you to explore the Internet for something by yourself as well. Who knows, perhaps you will find some learning course that fits you better. And that is great! However, when you find it, take extra 5-10 minutes to go through its curriculum and create a list with lessons.It feels so nice to cross out items off the todo list, believe me 😄Go through the official Study GuideDespite your findings on the Internet, I strongly suggest going through the official study guideStudy Guide - Terraform Associate CertificationIt took me about 20 hours to complete it (including practice tasks based on topics in the guide), and it was the core of my studying. I did not buy or search for some third-party course intentionally because I did have some Terraform experience before starting the preparation. But give the official guide a chance even if you found some course. It is well-made and matches real exam questions very precisely.Also, there is an official Exam Review. Someone might find this even better because it is a direct mapping of each exam objective to HashiCorp’s documentation and training.Take additional tutorialsHere is a list of additional tutorials and materials I suggest adding into your learning program:Official guides / documentation: Automate Terraform Collaborate using Terraform Cloud Terraform tutorials Reuse Configuration with Modules A Practitioner’s Guide to Using HashiCorp Terraform Cloud with GitHub Enforce Policy with SentinelThird-party articles and guides: Using the terraform console to debug interpolation syntax YouTube playlist with exam-like questions reviewFind yourself some practiceMockup a projectYou can greatly improve your practice by mocking some real business cases.If you already work in some company you can set up the project you’re working with using Terraform. If you don’t have a real project or afraid to accidentally violate NDA, try this open-source demo project: Real World Example Apps.It is a collection of different codebases for front-end and back-end used to build the same project. Just find the combination that suits your experience better and try to build the infrastructure for it using Terraform.Answer forum topicsLast but not least advice — try to answer some questions on the official Terraform forum.This is a nice way to test your knowledge, help others, and develop the community around Terraform. Just register there, look for the latest topics, and have fun!🍀 I sincerely wish you exciting preparation and a successful exam! 🍀", "url": "//2020/09/15/terraform-certification-tips.html" } , "2020-09-09-terraform-modules-explained-html": { "title": "What are terraform Modules and how do they work?", "tags": "terraform", "date": "September 9, 2020", "author": "", "category": "[&quot;Learning&quot;]", "content": "Surprisingly, a lot of beginners skip over Terraform modules for the sake of simplicity, or so they think. Later, they find themselves going through hundreds of lines of configuration code.I assume you already know some basics about Terraform or even tried to use it in some way before reading the article.Please note: I do not use real code examples with some specific provider like AWS or Google intentionally, just for the sake of simplicity.Terraform modulesYou already write modules even if you think you don’t.Even when you don’t create a module intentionally, if you use Terraform, you are already writing a module – a so-called “root” module.Any number of Terraform configuration files (.tf) in a directory (even one) forms a module.What does the module do?A Terraform module allows you to create logical abstraction on the top of some resource set. In other words, a module allows you to group resources together and reuse this group later, possibly many times.Let’s assume we have a virtual server with some features hosted in the cloud. What set of resources might describe that server? For example:– the virtual machine itself (created from some image)– an attached block device of specified size (for additional storage)– a static public IP mapped to the server’s virtual network interface– a set of firewall rules to be attached to the server– something else… (i.e. another block device, additional network interface, etc)Now let’s assume that you need to create this server with a set of resources many times. This is where modules are really helpful – you don’t want to repeat the same configuration code over and over again, do you?Here is an example that illustrates how our “server” module might be called.“To call a module” means to use it in the configuration file.Here we create 5 instances of the “server” using single set of configurations (in the module):1234567module \"server\" { count = 5 source = \"./module_server\" some_variable = some_value}Modules organisation: child and rootOf course, you would probably want to create more than one module. Here are some common examples: for a network (i.e. VPC) for a static content hosting (i.e. buckets) for a load balancer and it’s related resources for a logging configuration and whatever else you consider a distinct logical component of the infrastructureLet’s say we have two different modules: a “server” module and a “network” module. The module called “network” is where we define and configure our virtual network and place servers in it:123456789module \"server\" { source = \"./module_server\" some_variable = some_value}module \"network\" { source = \"./module_network\" some_other_variable = some_other_value}Once we have some custom modules, we can refer to them as “child” modules. And the configuration file where we call child modules relates to the root module.A child module can be sourced from a number of places: local paths official Terraform Registry (if you’re familiar with other registries, i.e. Docker Registry then you already understand the idea) Git repository (a custom one or GitHub/BitBucket) HTTP URL to .zip archive with moduleBut how can you pass resources details between modules?In our example, the servers should be created in a network. So how can we tell the “server” module to create VMs in a network which was created in a module called “network”?This is where encapsulation comes in.Module encapsulationEncapsulation in Terraform consists of two basic concepts: module scope and explicit resources exposure.Module ScopeAll resource instances, names, and therefore, resource visibility, are isolated in a module’s scope. For example, module “A” can’t see and does not know about resources in module “B” by default.Resource visibility, sometimes called resource isolation, ensures that resources will have unique names within a module’s namespace. For example, with our 5 instances of the “server” module:123module.server[0].resource_type.resource_namemodule.server[1].resource_type.resource_namemodule.server[2].resource_type.resource_nameOn the other hand, we could create two instances of the same module with different names:12345678module \"server-alpha\" { source = \"./module_server\" some_variable = some_value}module \"server-beta\" { source = \"./module_server\" some_variable = some_value}In this case, the naming or address of resources would be as follows:123module.server-alpha.resource_type.resource_namemodule.server-beta.resource_type.resource_nameExplicit resources exposureIf you want to access some details for the resources in another module, you’ll need to explicitly configure that.By default, our module “server” doesn’t know about the network that was created in the “network” module.So we must declare an output value in the “network” module to export its resource, or an attribute of a resource, to other modules.The module “server” must declare a variable to be used later as the input.This explicit declaration of the output is the way to expose some resource (or information about it) outside — to the scope of the ‘root’ module, hence to make it available for other modules.Next, when we call the child module “server” in the root module, we should assign the output from the “network” module to the variable of the “server” module:1network_id = module.network.network_idHere is how the final code for calling our child modules will look like in result:1234567891011module \"server\" { count = 5 source = \"./module_server\" some_variable = some_value network_id = module.network.network_id}module \"network\" { source = \"./module_network\" some_other_variable = some_other_value}This example configuration would create 5 instances of the same server, with all the necessary resources, in the network we created with as a separate module.Wrap upNow you should understand what modules are and what do they do.If you’re at the beginning of your Terraform journey, here are some suggestions for the next steps.I encourage you to take this short tutorial from HashiCorp, the creators of Terraform, about modules: “Organize Configuration”Also, there is a great comprehensive study guide which covers everything from beginner to advanced concepts about Terraform: “Study Guide - Terraform Associate Certification”The modular code structure makes your configuration more flexible and yet easy to be understood by others. The latter is especially useful in teamwork.This article was originaly published on FreeCodeCamp paltform by me, but I still want to keep it here for the record. Canonical link to original publication was properly set in the page headers.", "url": "//2020/09/09/terraform-modules-explained.html" } , "2020-08-25-terraform-cli-shortcuts-html": { "title": "Terraform CLI shortcuts", "tags": "terraform, cli, automation", "date": "August 25, 2020", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "Here is some CLI shortcuts I use day-to-day to simplify and speed-up my Terraform workflow.Requirements — bash-compatible interpreter, because aliases and functions described below will work with bash, zsh and ohmyzsh.In order to use any of described aliases of functions, you need to place it in your ~/.bashrc or ~/.zshrc file (or any other configuration file you have for your shell).Then just source this file, for example: source ~/.zshrcFunction: list outputs and variables of given moduleYou need to provide the path to module directory, and this function will list all declared variables and outputs module has. It comes very useful when you don’t remember them all and just need to take a quick look.## TerraForm MOdule Explainedfunction tfmoe { echo -e \"\\nOutputs:\" grep -r \"output \\\".*\\\"\" $1 |awk '{print \"\\t\",$2}' |tr -d '\"' echo -e \"\\nVariables:\" grep -r \"variable \\\".*\\\"\" $1 |awk '{print \"\\t\",$2}' |tr -d '\"'}Example usage:1234567891011user@localhost $: tfmoe ./module_albOutputs:\t alb_arnVariables:\t acm_certificate_arn\t lb_name\t alb_sg_list\t subnets_id_list\t tagsFunction: pre-fill module directory with configuration filesYou need to provide a path to the module directory and this function will create a bunch of empty ‘default’ .tf files in it.1234567#TerraForm MOdule Initializefunction tfmoi { touch $1/variables.tf touch $1/outputs.tf touch $1/versions.tf touch $1/main.tf}Example usage:1234user@localhost $: mkdir ./module_foo &amp;&amp; temoi $_user@localhost $: ls ./module_foomain.tf outputs.tf variables.tf versions.tfAliasesThe purpose of these aliases is just to keep you from typing long commands when you want to do a simple action.1234567alias tf='terraform'alias tfv='terraform validate'alias tfi='terraform init'alias tfp='terraform plan' This one is useful because it makes format tool to go in-depth (recursively) through directories.1alias tfm='terraform fmt -recursive'Example usage:123user@localhost $: tfm module_ecs_cluster/ecs.tfmodule_alb/alb.tf", "url": "//2020/08/25/terraform-cli-shortcuts.html" } , "2020-08-06-ansible-secrets-aws-ssm-sm-html": { "title": "Managing Ansible playbook secrets with AWS services", "tags": "ansible, aws, devops, security", "date": "August 6, 2020", "author": "", "category": "[&quot;Technical Blogs&quot;]", "content": "Lookup plugins for Ansible allow you to do a lot of cool things. One of them is to securely pass sensitive information to your playbooks. If you manage some apps in AWS with Ansible, then using Parameter Store or Secrets Manager along with it might greatly improve your security.Variables with SSM Parameter StoreLet’s say you have some variables defined in ‘defaults/main.yaml’ file of your role or maybe in group_vars.yaml file.123456---# content of dev.vars.yaml to be included in your play or roleuse_tls: trueapplication_port: 3000app_env: developmentstripe_api_key: 1HGASU2eZvKYlo2CT5MEcnC39HqLyjWDIf you store such things locally on Ansible control node, you probably encrypt it with ansible-vaultSSM Parameter Store gives you more flexibility and security by centralized storage and management of parameters and secrets, so let’s use it with Ansible:123456---# content of dev.vars.yaml to be included in your play or roleuse_tls: \"{{lookup('aws_ssm', '/dev/webserver/use_tls')}}\"application_port: \"{{lookup('aws_ssm', '/dev/webserver/application_port')}}\"app_env: \"{{lookup('aws_ssm', '/dev/webserver/app_env')}}\"stripe_api_key: \"{{lookup('aws_ssm', '/dev/webserver/stripe_api_key')}}\"The syntax is fairly simple:The aws_ssm argument – is the name of plugin.The /dev/webserver/use_tls argument – is the path to the key in Paramter Store.Surely you can do the same for a group of servers with group variables, for example:You can use this anywhere you can use templating: in a play, in variables file, or a Jinja2 template.Variables with Secret ManagerAnother cool lookup plugin is Secrets Manager. In a nutshell, it has the same kind of functionality but it uses JSON format by feault.Here is a quick example of its functionality in a Playbook:1234---- name: Extract something secrets from Secret Manager debug: msg: \"{{ lookup('aws_secret', 'dev/some-secrets')}}\"The above task will generate the following output1234567891011TASK [Extract something secrets from Secret Manager] ****************************************************ok: [some_server] =&gt; { \"msg\": { \"dbname\": \"database\", \"engine\": \"mysql\", \"host\": \"127.0.0.1\", \"password\": \"password\", \"port\": \"3306\", \"username\": \"db_user\" }}This is nice if you want to insert a JSON as is, but you will need additional parsing in case you want to get only some of JSON elements.ConclusionIf you’re using Ansible in CI/CD, then having it on an EC2 Instance with the IAM role will make you avoid keeping any secrets on that instance at all.The IAM role must allow at least the read access to SSM Parameter Store (+ KMS read access to be able to decrypt the keys) or the read access to Secrets Manager.You can find documentation for described plugins here aws_ssm and here aws_secret.More about lookup plugins: https://docs.ansible.com/ansible/latest/plugins/lookup.html", "url": "//2020/08/06/ansible-secrets-aws-ssm-sm.html" } , "2020-05-02-terraform-explained-for-managers-html": { "title": "Terraform explained for managers", "tags": "terraform, devops", "date": "May 2, 2020", "author": "", "category": "[&quot;Learning&quot;]", "content": "This article was written a long time ago in a galaxy far, far away…As a team leader, I have to speak with my teammates on the same language — technical language…For example, I have a good technical background, yet sometimes I have a feeling that my teammates see that I don’t understand them when we discuss some project or a task in-depth. Moreover, I know they are right. Of course, there are plenty of managers who do not have a technical background and they perform great. And some might say that technical skills are not the priority for a manager.But I think that +1 to skills is always better than +0. Of course, it is a question of time and personal interests, one way or another.This is why I decided to share my experience and explain Terraform in one blog post.The language of this article will be ‘techie’ but not too much. This is because I want to highlight the main parts the Terraform consists of. Although, this is not technical documentation (I hope). Code examples will be based on AWS cloud configuration, although in-depth knowledge of AWS is not required to understand them.A few words about “Infrastructure as Code”IaC is when you describe and manage your infrastructure as… (guess what?) …code. Literally.In a nutshell that means you can define all the elements (servers, networks, storage, etc.) and resources (memory, cpu, etc) of your infrastructure via configuration files in the version control system (Git, SVN, etc.), and manage it in a way similar to how you manage the source code of the applications: branches, releases, and all that stuff.And the main idea behind the IaC approach is that it manages the state of things and must be the single source of truth (configuration truth) for your infrastructure. You define the state via the code (at first) and then IaC tool (Terraform, for example) applies this state on the infrastructure: all that is missing according to the code will be created, all that differs from the code will be changed and all that exists in the infrastructure but is not described via code — will be destroyed.Why and when do you need the Terraform for a project?Terraform is a specific tool, hence like any other tool it has its particular application area. There is no strict definition of project kind that needs Terraform (surprise!) but in general, you need to consider using Terraform if you answer ‘yes’ to one of the following questions: Do you have multiple logical elements of the same kind (in plural) in your infrastructure, i.e. several web servers, several application servers, several database servers? Do you have numerous environments (or workspaces) where you run your applications, i.e. development, staging, QA, production? Do you spend some significant amount of time managing the changes in the environment(s) where you run your applications?How does it work?Terraform works with the source code of configuration, and interprets the code into real resources inside on-premise or cloud platforms.Terraform supports a lot of platforms: from major cloud providers such as AWS, Azure, GCP, DigitalOcean, to more modest platforms such as OVH, 1&amp;1, Hetzner, and others. It also supports infrastructure software such as Docker, Kubernetes, Chef, and even databases and monitoring software. This is why Terraform is so popular — it is a real Swiss knife in the operations world.So to create, change, or destroy the infrastructure Terraform needs the source code. The source code is a set of configuration files that defines your infrastructure state. The code uses its own syntax but it looks very user friendly. Here is an example: the following configuration block describes the virtual server (EC2 instance) in AWS1234resource \"aws_instance\" “web_server” { ami = \"ami-a1b2c3d4\" instance_type = \"t3.micro\" }Terraform can automatically detect the dependencies between resources described in the code, and also allows you to add custom dependencies when needed.When you apply the code first time, Terraform creates a so-called “state file” that works as a mapping of your code to real resources created in the hosting platform. With each next “apply” action Terraform will use it to compare the code changes with the sate file to decide what should be done (and in what order) against real infrastructure.One of the important functions of the state file is a description of dependencies between the resources. For example (some technical nuances are omitted for purpose of simplicity): if you have a server created inside some network and that network is going to be changed, then Terraform will know that server setting should be changed as well or server should be re-created inside the updated network.What is inside?Terraform configuration code consists of several elements: providers, resources, modules, input variables, output values, local values, expressions, functions.Provider is an entity that defines what exactly is possible to do with cloud or on-premises infrastructure platform you manage via Terraform.Resource is the most important part of the configuration code. This is where the definition of infrastructure objects happens. Resources are the main building blocks of the whole code.Every resource has a type and local name. For example here is how EC2 instance configuration may look like:1234resource “aws_instance” “web_server” { ami = “ami-a1b2c3d4” instance_type = “t3.micro” }The aws_instance is a resource type and web_server is the resource local name. Later, when Terraform applies this code, it will create an EC2 instance with some particular ID in AWS. Once created, the ID will be stored in the state file with mapping information that logically connects it with web_server.The ami, instance_type and private_ip are the arguments with values which define the actual state of the resource. There are plenty of value types, depending on the particular argument and particular resource type, so I will not focus on them here.Modules are the kind of logical containers or groups for resources you define and use together. The purpose of modules is not only the grouping of resources but it is also the possibility to reuse the same code with different variables.Let’s get back to the example with EC2 instance and say you need to have a static public IP address with it. In such a case, here is how the module for web server may look like:1234567resource “aws_instance” “web_server” { ami = “ami-a1b2c3d4” instance_type = “t3.micro” }resource “aws_eip” “web_server_public_ip” { instance = “${aws_instance.web_server.id}” }Having these two resources together allows us to think of it as a stand-alone unit you can reuse later, for example in our development, staging, and production environments. And not by copying and pasting it, but via reference to the module defined only once.Please note: we specified an instance argument inside the aws_eip resource as a reference to another resource details (the ID of an instance). This is possible because of a way how Terraform treats dependencies: when it detects the dependency (or you define it explicitly) it will create the main resource first, and only after it’s created and available it will create the dependent one.Input variables work as parameters for the modules so module code could be reusable. Let’s look at the previous example: it has some hardcoded values — instance image ID and instance type. Here is how you can make it more abstract and reusable:12345678910variable “image_id” { type = string }variable “instance_type” { type = string }resource “aws_instance” “web_server” { ami = var.image_id instance_type = var.instance_type }Values for the variables then can be passed either via CLI and environment variables (if you have only the one, so-called root module) or via explicit values in the block where you call a module, for example:12345678910module “web_server_production” { source. = “./modules/web_server” image_id = “ami-a1b2c3d4” instance_type = “m5.large” }module “web_server_development” { source = “./modules/web_server” image_id = “ami-a2b3c4d5” instance_type = “t3.micro” }Output values are similar to the “return” of a function in development language. They can be used for dependencies management (for example, when a module require something from another module) and for the printing of the certain values at the end of Terraform work (for example to be used for notification in CI/CD process).Local values, expressions, functions — three more things that augment the capabilities of Terraform and make it more similar to a programming language (which is great by the way).The local values are used inside modules for extended data manipulations in it.The expressions are used to set the values (for many things), for example, to set the value of some argument in resource configuration. They used either to refer something (just as we referenced instance ID “${aws_instance.web_server.id}” in the example above) or to compute the value within your configuration.The functions in Terraform are built-int jobs you can call to transform and combine values. For example, the tolist() function converts its argument to a list value.And this is it?Yes, in a very very short words — this is what Terraform is. Not a rocket science if it’s about to manage a small infastructure, but gets more complicated with bigger infrastctucture. As any other engineerign tool or development language, actually.Okay, what next?If you read down to this point (anybody?) then it means it worth “get your hands dirty” and to try building your Infrastructure with Terraform. There are plenty of courses and books (and the “Terraform up and running” is one of the most popular), but my learning path started from the following: Official guide from Hashicorp — great and free guide from Terraform developers. Just pick your favorite cloud (AWS, Azure, GCP) and go through the topics.Once you finish this guide, I suggest jumping into the more real-world things and describe the infrastructure of the most common project you work with. For example, here is what I do: small github project – I am trying to describe the Infrastructure for SPA website with services in docker containers at the backend. The variety and complexity of the code are limited only by your fantasy.Another thing worth your attention is A Comprehensive Guide to Terraform.And I also encourage you to go through the collection of blog posts and talks they share.", "url": "//2020/05/02/Terraform-explained-for-managers.html" } , "2020-03-20-devops-team-in-outsource-a-team-html": { "title": "DevOps team in outsource. A team?", "tags": "", "date": "March 20, 2020", "author": "", "category": "[&quot;Mindset&quot;]", "content": "This article was written a long time ago in a galaxy far, far away…I am a team lead in an outsourcing company. That means I believe (or convince myself) that I lead the team (wait, wait, there will be even more obvious discoveries).I am sure that a team can achieve more than a single person or a group. And the story I want to tell is about making the team from a group of people with personal and non-connected tasks. I assume this is going to be a series of posts, and hopefully, I will add a table of contents here.A group or a team?It is quite rare to find a project that involves more than 10-15 people in a small or mid-size outsourcing company. With such project size, a single DevOps engineer is enough to handle the whole job. Or an engineer may work on several projects at the same time if projects are relatively small. This is because the scope of work to be done fits into the schedule of a single person. So it is either one-to-one or one-to-many connection in most cases: either the employee works with one project or the employee works with several projects simultaneously.Outsourcing a “DevOps” within this context usually means assigning an individual for the work with a client or/and a client’s team.Now imagine: you have several people in a room, all of them are DevOps engineers working in the same outsource company and all they work with their “own” projects separately.So would you call that group of people “a team”?Wikipedia states that “a team” is a group of individuals (human or non-human) working together to achieve their [common] goal. As well as common sense tells us that “a team” is something that groups people to make them work for a mutual outcome.And here comes an obvious but a difficult question: what common goal(s) a group of people can have whereas each member of that group has its own goal, which is not related to others’? A bit contradictory, isn’t it?A GoalConventional thinking tells us that a goal for the for-profit company is to make money. This is true, and I don’t want to argue with it. Therefore, does it mean that a goal for a team of engineers inside the company is the same as for the company as a whole?Yes… Yet, not only, or almost.Yes, because it’s the main reason why we work for an outsourcing company. We do the job for clients, clients pay the company, the company pays us.However, there is a long chain of events and processes that leads from the start of a contract to the paid invoice. A team of engineers is the inalienable part of that chain, with a focus on its work - deliver technical solutions following the client’s expectations and requirements.This conclusion reveals the goal for the DevOps team in the outsourcing company: grow the quality of work by a process of ongoing improvement and refinement of technical and soft skills. So simple, so obvious.And this goal means nothing by itself. But to sustain it and adapt, to make it desirable for the team, and to make it a team’s foundation eventually, we need the following: Values Principles Common interestsOne of the crucial responsibilities of a team lead is to foster these three pillars of a team.The VALUES is something that lets us feel comfortable with the people surrounding us in the office. The list of values is not carved in stone, it can (and eventually should) change as the team evolves. But here is the most constant items for my team: openness in communication respect to each other’s opinion empathy feeling free to provide critic and being ready to receive it count on help from a teammate and be ready to help ability to work remotely and have flexible working hoursThe PRINCIPLES is a set of beliefs and policies that keeps us on the required level of productivity and disciplines us, and stimulates the longing to improve. Again, this list can be and should be adjusted as the time goes and the team evolves, but here is the most up to date for us: we learn constantly we are the first who appraises the quality of our work we are proactive we are open: if we do not agree with something or something is worrying us, we should be brave enough to say it we may defend own point of view, but be ready to accept that it is incorrect we can make mistakes, but we must learn from it and do not repeat them we must help our teammates we keep our promisesThe COMMON INTERESTS is more about informal things we share within a team. This includes (but not limited to) team building events, discussions and arguments about new technologies, so-called water-cooler chats, and so on. Being less formal, this pillar acts as a glue for the team. This is because we are all humans, first of all, and we generally work for 1/3 of our day (not including weekends), so we need to stay humans at work as well. I mean it is impossible to switch off the ‘human’ for that 1/3 of the day, leaving only the engineer for this time. Hence, this must be admitted and we must cope with it by putting several informal things into formal work.These pillars create a team culture where learning, creativity, and an open mindset are encouraged.The TeamThis is how a group of people with separate goals may become a team. While we do not have a goal (“one for all”) that is measurable or has a fixed point of completion, we still have a common aim whose realization is based on pillars of teamwork. And with a set of sub-goals, this may be a specific, achievable (until you find something else to improve, which should happen quite often) and even realistic. So it’s kinda 60% S.M.A.R.T. goal :smile:Unfortunately or fortunately (if you like your team lead job) there is a lot of work that needs to be done and a lot of questions need to be answered to implement the described approach: How to translate own values to the team and create the new ones together? How to make people communicate in such a team and share their knowledge? How to define who does what? How to teach team members to learn? How to keep team members loyal to the team and company? How to mitigate conflicts and how to use them? How to keep team members motivated when the work becomes boring and the level of engagement goes down? and much much moreI wish I could answer all these questions at once, and I wish there were only correct answers.But my experience tells me that it is possible to answer them one by one. And I hope to share that here someday!", "url": "//2020/03/20/devops-team-in-outsource-a-team.html" } , "2020-03-18-github-actions-first-impression-html": { "title": "Github Actions - First impression", "tags": "github, automation", "date": "March 18, 2020", "author": "", "category": "[&quot;Review&quot;]", "content": "Although Github Actions service is generally available since November 13, 2020, and there are about 243,000,000 results for “github actions” in Google search already, I have just reached it…It’s half past midnight, it took me about 35 commits to make my first github automation work, but it finally works and this blog post was built and published automatically!Actions everywhereOne of the most (or maybe the most one) powerful things in Actions is … Actions! Github made a simple but genius thing: they turned well-known snippets (we do with pipelines) into the marketplace of well-made (sometimes not) simple and complex applications you can use in your automation workflow. https://github.com/marketplace?type=actionsSo now you can either re-invent your wheel or re-use someone else’s code to make the needed automation.I decided to automate publications to this blog via Actions in order to have some practice.There are two workflows: one for the blog (website), and one for the CV (cv). actions/checkout@v2 actions/upload-artifact@v2 actions/download-artifact@v2In both workflows, the build job is performed within a container, which is different per workflow: Ruby for the blog and Pandoc for CV.Here is how the build job looks like for the blog:jobs: build: runs-on: ubuntu-latest container: image: ruby:2.6.4 options: --workdir /src steps: - name: Checkout uses: actions/checkout@v2 - name: Build blog run: | bundle install bundle exec jekyll build --verbose --destination _site - name: Upload artifacts uses: actions/upload-artifact@v2 with: name: _site path: _siteAs you can see, I run the steps within the Ruby container. This simplifies things related to file permissions and directory mounting because checkout is made inside the container.The deploy step is performed via shell run command for now, for better clearness (can be replaced to third-party action or custom-made one): it makes a commit to gh-pages branch which is configured for Github Pages.123456789101112131415161718192021222324deploy: if: github.ref == 'refs/heads/master' needs: build runs-on: ubuntu-latest steps: - name: Checkout gh-pages branch uses: actions/checkout@v2 with: ref: 'gh-pages' - name: Get the build artifact uses: actions/download-artifact@v2 with: name: _site path: ./ - name: Deploy (push) to gh-pages run: | git config user.name \"$GITHUB_ACTOR\" git config user.email \"${GITHUB_ACTOR}@bots.github.com\" git add -A git commit -a -m \"Updated Website\" git remote set-url origin \"https://x-access-token:$@github.com/vasylenko/serhii.vasylenko.info.git\" git push --force-with-lease origin gh-pagesOld good things made betterA lot of common things have been introduced to GitHubActions with some sweet additions: you can also specify different environments for your jobs in the same workflow; you can use environment variables with a different visibility scope: either workflow, or job, or step; you can use cache for dependencies and reuse it between workflow runs while keeping workflow directory clean; you can trigger a workflow by repo events and have a quite complex conditional logic or filters (if needed), external webhooks and by a schedule; you can pass artifacts between jobs inside a workflow with ease - Github provides simple actions for this, so you don’t need to dance around temporary directories or files; and much more", "url": "//2020/03/18/github-actions-first-impression.html" } , "2020-03-15-aws-solutions-architect-associate-exam-tips-html": { "title": "AWS SAA exam results", "tags": "aws", "date": "March 15, 2020", "author": "", "category": "[&quot;Learning&quot;]", "content": "926 out of 1000Last week I’ve successfully passed AWS SAA exam with 926 points from 1000 possible. I can’t help saying this and showing off my verification page, just because I am very happy so please excuse me my bragging.What helped meBut I would like to share some advices and tips with anyone who reads this and wants to pass the exam. I mean, I could just twit about it if that was only about saying “hey look at me!”, right?It took me a month of intensive studying and here is what helped me: Video course at CloudGuru - AWS Certified Solutions Architect Associate. Price: $50 for a monthly subscription. Tips: They have a 7 days free trial, which is actually quite enough to view the whole course. But I strongly recommend purchasing a full month, because it is better to view the lectures gradually during couple of weeks for better learning. Plus they have a nice exam simulator where you can practice several times. Practice Tests set at Udemy - AWS Certified Solutions Architect Associate Practice Exams. Price: $40 or only $12 if you’re lucky to get it during a sale. But they make sales quite often and they frequently provide discuounts for new students. I purchaced it for $12. Tips: practice tests are very useful, do not skip buying them. You will find your weak spots and also learn a lot by passing these tests. This particular set has a quite good explanations for each question. Exam Guide at O’relly Media AWS Certified Solutions Architect Associate All-in-One Exam Guide. Price: this one can be easily read during 10 days free trial period :wink: Tips: The new exam version is released on 23rd of March, so it is better to find a new updated version of exam guide. And I suggest reading the guide after the video course or vise versa, but do not mix them. Making notes. Seriously, note taking helps you memorize better. Do not skip it, and note your video courses as well as exam guide. Later, you will find your notes very helpful before the exam day - they will fresh up your memory. Thank you for reading down to this point. I hope my advices were helpful and you will pass the exam!", "url": "//2020/03/15/aws-solutions-architect-associate-exam-tips.html" } , "2019-08-10-customize-the-favicon-html": { "title": "📚 Customize the Favicon", "tags": "favicon", "date": "August 10, 2019", "author": "Cotes Chung", "category": "", "content": "In Chirpy, the image files of Favicons are placed in assets/img/favicons/. You may need to replace them with your own. So let’s see how to customize these Favicons.With a square image (PNG, JPG or GIF) in hand, open the site Favicon &amp; App Icon Generator and upload your original image.Click button Create Favicon and wait a moment for the website to generate the icons of various sizes automatically.Download the generated package, unzip and delete the following two from the extracted files: browserconfig.xml manifest.jsonNow, copy the remaining image files (.PNG and .ICO) from the extracted .zip file to cover the original files in the folder assets/img/favicons/.The following table helps you understand the changes to the icon file: ✓ means keep, ✗ means delete. File(s) From Favicon &amp; App Icon Generator From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ browserconfig.xml ✗ ✓ manifest.json ✗ ✓ The next time you build the site, the icon will be replaced with a customized edition.", "url": "//2019/08/10/customize-the-favicon.html" } , "2019-08-09-getting-started-html": { "title": "📚 Getting Started", "tags": "getting started", "date": "August 9, 2019", "author": "Cotes Chung", "category": "", "content": "InstallationFork Chirpy on GitHub, rename the repository to USERNAME.github.io (where USERNAME is your GitHub username), and then open terminal and clone the fork to local by:1$ git clone https://github.com/USERNAME/USERNAME.github.io.git -b master --single-branchSetting up the local envrionmentIf you would like to run or build the project on your local machine, please follow the Jekyll Docs to complete the installation of Ruby, RubyGems, Jekyll and Bundler.Before running or building for the first time, please complete the installation of the Jekyll plugins. Go to the root directory of project and run:1$ bundle installbundle will automatically install all the dependencies specified by Gemfile.Setting up Docker environment (optional)If you’re a loyal fan of Docker or just too lazy to install the packages mentioned in Setting up the local envrionment, please make sure you have Docker Engine installed and running, and then get Docker image jekyll/jekyll from Docker Hub by the following command:1$ docker pull jekyll/jekyllUsageInitializationGo to the root directory of the project and start initialization:1$ bash tools/init.sh Note: If you not intend to deploy it on GitHub Pages, append parameter option --no-gh at the end of the above command.What it does is: Remove some files or directories from your repository: .travis.yml files under _posts folder docs If you use the --no-gh option, the directory .github will be deleted. Otherwise, setup the GitHub Action workflow by removing extension .hook of .github/workflows/pages-deploy.yml.hook, and then remove the other files and directories in folder .github. Automatically create a commit to save the changes. ConfigurationGenerally, go to _config.yml and configure the variables as needed. Some of them are typical options: url avatar timezone theme_modeRun LocallyYou may want to preview the site contents before publishing, so just run it by:1$ bundle exec jekyll sThen open a browser and visit to http://localhost:4000.Run on DockerRun the site on Docker with the following command:1234$ docker run --rm -it \\ --volume=\"$PWD:/srv/jekyll\" \\ -p 4000:4000 jekyll/jekyll \\ jekyll serveDeploymentBefore the deployment begins, checkout the file _config.yml and make sure the url is configured correctly. Furthermore, if you prefer the project site and don’t use a custom domain, or you want to visit your website with a base url on a web server other than GitHub Pages, remember to change the baseurl to your project name that starting with a slash. For example, /project.Assuming you have already gone through the initialization, you can now choose ONE of the following methods to deploy your website.Deploy on GitHub PagesFor security reasons, GitHub Pages build runs on safe mode, which restricts us from using plugins to generate additional page files. Therefore, we can use GitHub Actions to build the site, store the built site files on a new branch, and use that branch as the source of the Pages service. Push any commit to origin/master to trigger the GitHub Actions workflow. Once the build is complete and successful, a new remote branch named gh-pages will appear to store the built site files. Browse to your repo’s landing page on GitHub and select the branch gh-pages as the publishing source throught Settings → Options → GitHub Pages: Visit your website at the address indicated by GitHub. Deploy on Other PlatformsOn platforms other than GitHub, we cannot enjoy the convenience of GitHub Actions. Therefore, we should build the site locally (or on some other 3rd-party CI platform) and then put the site files on the server.Go to the root of the source project, build your site by:1$ JEKYLL_ENV=production bundle exec jekyll bOr, build the site with Docker by:12345$ docker run -it --rm \\ --env JEKYLL_ENV=production \\ --volume=\"$PWD:/srv/jekyll\" \\ jekyll/jekyll \\ jekyll buildUnless you specified the output path, the generated site files will be placed in folder _site of the project’s root directory. Now you should upload those files to your web server.", "url": "//2019/08/09/getting-started.html" } , "2019-08-08-write-a-new-post-html": { "title": "📚 Writing a New Post", "tags": "writing", "date": "August 8, 2019", "author": "Cotes Chung", "category": "", "content": "Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _post/ of the root directory. Please note that the EXTENSION must be one of md and markdown.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:123456---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- Note: The posts’ layout has been set to post by default, so there is no need to add the variable layout in Front Matter block.Timezone of dateIn order to accurately record the release date of a post, you should not only setup the timezone of _config.yml but also provide the the post’s timezone in field date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post is designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:12categories: [Animal, Insect]tags: [bee]Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for specific post, add the following to post’s Front Matter:123---toc: false---CommentsSimilar to TOC, the Disqus comments is loaded by default in each post, and the global switch is defined by variable comments in file _config.yml . If you want to close the comment for specific post, add the following to the Front Matter of the post:123---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:123---math: true---MermaidMermaid is a great diagrams generation tool. To enable it on your post, add the following to the YAML block:123---mermaid: true---Then you can use it like other markdown language: surround the graph code with ```mermaid.ImagesPreview imageIf you want to add an image to the top of the post contents, specify the url for the image by:123---image: /path/to/image-file---Image captionAdd italics to the next line of an image，then it will become the caption and appear at the bottom of the image:12![img-description](/path/to/image)_Image Caption_Image sizeYou can specify the width (and height) of a image with width:1![Desktop View](/assets/img/sample/mockup.png){: width=\"400\"}Image positionBy default, the image is centered, but you can specify the position by using one of class normal , left and right. For example: Normal position Image will be left aligned in below sample: 1![Desktop View](/assets/img/sample/mockup.png){: width=\"350\" class=\"normal\"} Float to the left 1![Desktop View](/assets/img/sample/mockup.png){: width=\"240\" class=\"left\"} Float to the right 1![Desktop View](/assets/img/sample/mockup.png){: width=\"240\" class=\"right\"} Limitation: Once you specify the position of an image, it is forbidden to add the image caption.Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:123---pin: true---Code BlockMarkdown symbols ``` can easily create a code block as following examples.1This is a common code snippet, without syntax highlight and line number.Specific LanguageUsing ```language you will get code snippets with line numbers and syntax highlight. Note: The Jekyll style {% highlight LANGUAGE %} or {% highlight LANGUAGE linenos %} are not allowed to be used in this theme !123456# Yaml code snippetitems: - part_no: A4786 descrip: Water Bucket (Filled) price: 1.47 quantity: 4Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %} .123{% if product.title contains 'Pack' %} This product's title contains the word Pack.{% endif %}Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts.", "url": "//2019/08/08/write-a-new-post.html" } , "2019-08-08-text-and-typography-html": { "title": "📚 Text and Typography", "tags": "", "date": "August 8, 2019", "author": "Cotes Chung", "category": "", "content": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.TitlesH1 - headingH2 - headingH3 - headingH4 - headingParagraphI wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.ListOrdered list Firstly Secondly ThirdlyUnordered list Chapter Setcion Paragraph Checkbox list TODO Completed Hold on Defeat COVID-19 Vaccine production Economic recovery People smile again Block Quote This line to shows the Block Quote.Tables Company contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkhttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1.Images Default (with caption)Full screen width and center alignment Specify width400px image width Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dInline codeThis is an example of Inline Code.MathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When \\(a \\ne 0\\), there are two solutions to \\(ax^2 + bx + c = 0\\) and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Code SnippetCommon1This is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole12$ dateSun Nov 3 15:11:12 CST 2019Terminal123$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashRuby1234def sum_eq_n?(arr, n) return true if arr.empty? &amp;&amp; n == 0 arr.product(arr).reject { |a,b| a == b }.any? { |a,b| a + b == n }endShell1234if [ $? -ne 0 ]; then echo \"The command was not successful.\"; #do the needful / exitfi;Liquid123{% if product.title contains 'Pack' %} This product's title contains the word Pack.{% endif %}Html123456789101112&lt;div class=\"sidenav\"&gt; &lt;a href=\"#contact\"&gt;Contact&lt;/a&gt; &lt;button class=\"dropdown-btn\"&gt;Dropdown &lt;i class=\"fa fa-caret-down\"&gt;&lt;/i&gt; &lt;/button&gt; &lt;div class=\"dropdown-container\"&gt; &lt;a href=\"#\"&gt;Link 1&lt;/a&gt; &lt;a href=\"#\"&gt;Link 2&lt;/a&gt; &lt;a href=\"#\"&gt;Link 3&lt;/a&gt; &lt;/div&gt; &lt;a href=\"#contact\"&gt;Search&lt;/a&gt;&lt;/div&gt;Java12345678910111213141516private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException { // Write out any hidden serialization magic s.defaultWriteObject(); // Write out HashMap capacity and load factor s.writeInt(map.capacity()); s.writeFloat(map.loadFactor()); // Write out size s.writeInt(map.size()); // Write out all elements in the proper order. for (E e: map.keySet()) s.writeObject(e);}Reverse Footnote The footnote source. &#8617; ", "url": "//2019/08/08/text-and-typography.html" } }; </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.6/lunr.min.js"></script> <script src="/assets/js/search.js"></script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/vasylenko">Serhii Vasylenko</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div><script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script> <noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt=""/></noscript></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/aws/">aws</a> <a class="post-tag" href="/tags/terraform/">terraform</a> <a class="post-tag" href="/tags/automation/">automation</a> <a class="post-tag" href="/tags/devops/">devops</a> <a class="post-tag" href="/tags/ansible/">ansible</a> <a class="post-tag" href="/tags/macos/">macos</a> <a class="post-tag" href="/tags/cli/">cli</a> <a class="post-tag" href="/tags/cloudfront/">cloudfront</a> <a class="post-tag" href="/tags/favicon/">favicon</a> <a class="post-tag" href="/tags/fun/">fun</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://serhii.vasylenko.info{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
